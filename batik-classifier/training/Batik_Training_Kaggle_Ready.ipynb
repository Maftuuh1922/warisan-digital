{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753b4898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üéÆ CHECK GPU - PASTIKAN GPU P100 AKTIF!\n",
    "# ============================================================\n",
    "import subprocess\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üéÆ CHECKING GPU AVAILABILITY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"\\n‚úÖ GPU DETECTED!\")\n",
    "        print(result.stdout)\n",
    "        \n",
    "        # Check if it's P100\n",
    "        if 'P100' in result.stdout:\n",
    "            print(\"\\nüéâ PERFECT! GPU P100 aktif!\")\n",
    "        elif 'T4' in result.stdout:\n",
    "            print(\"\\n‚úÖ OK! GPU T4 aktif (sedikit lebih lambat dari P100)\")\n",
    "        else:\n",
    "            print(\"\\n‚úÖ GPU aktif!\")\n",
    "    else:\n",
    "        raise Exception(\"nvidia-smi failed\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚ö†Ô∏è  WARNING: GPU TIDAK TERDETEKSI!\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nüìã CARA AKTIFKAN GPU:\")\n",
    "    print(\"1. Klik icon Settings (‚öôÔ∏è) di kanan atas\")\n",
    "    print(\"2. Accelerator: Pilih 'GPU P100'\")\n",
    "    print(\"3. Klik 'Save'\")\n",
    "    print(\"4. Notebook akan restart, jalankan cell ini lagi\")\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"\\n‚ö†Ô∏è  TRAINING TANPA GPU AKAN SANGAT LAMBAT!\")\n",
    "    print(\"   Estimasi: 40-50 jam vs 6-8 jam dengan GPU P100\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b79978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üì¶ EXTRACT DATASET - UNZIP BATIK ULTIMATE\n",
    "# ============================================================\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üì¶ EXTRACTING BATIK ULTIMATE DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Kaggle dataset path\n",
    "dataset_zip = '/kaggle/input/batik-teist/batik_ultimate.zip'\n",
    "extract_to = '/kaggle/working/batik_ultimate'\n",
    "\n",
    "if os.path.exists(dataset_zip):\n",
    "    print(f\"\\n‚úÖ Found: {dataset_zip}\")\n",
    "    print(f\"üìÇ Extracting to: {extract_to}\")\n",
    "    print(\"\\n‚è≥ This will take 2-3 minutes...\\n\")\n",
    "    \n",
    "    # Extract\n",
    "    with zipfile.ZipFile(dataset_zip, 'r') as zip_ref:\n",
    "        zip_ref.extractall('/kaggle/working')\n",
    "    \n",
    "    print(\"\\n‚úÖ Extraction complete!\")\n",
    "    \n",
    "    # Count files\n",
    "    total_images = 0\n",
    "    total_classes = 0\n",
    "    \n",
    "    if os.path.exists(extract_to):\n",
    "        for root, dirs, files in os.walk(extract_to):\n",
    "            if not dirs:  # Leaf directory\n",
    "                img_files = [f for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "                if img_files:\n",
    "                    total_images += len(img_files)\n",
    "                    total_classes += 1\n",
    "        \n",
    "        print(f\"\\nüìä Dataset Statistics:\")\n",
    "        print(f\"   Total classes: {total_classes}\")\n",
    "        print(f\"   Total images: {total_images}\")\n",
    "        print(f\"   Avg per class: ~{total_images // total_classes if total_classes > 0 else 0}\")\n",
    "        \n",
    "        if total_images > 20000:\n",
    "            print(\"\\nüéâüéâüéâ PHENOMENAL DATASET!\")\n",
    "            print(\"   Expected accuracy: 95-99%+ üöÄüöÄüöÄ\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  Warning: Extract folder not found!\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Dataset not found: {dataset_zip}\")\n",
    "    print(\"\\nüìã CARA FIX:\")\n",
    "    print(\"1. Klik '+ Add Data' di kanan atas\")\n",
    "    print(\"2. Search: 'batik-teist' atau 'muhammadmaftuh'\")\n",
    "    print(\"3. Klik dataset 'Batik Teist'\")\n",
    "    print(\"4. Klik 'Add'\")\n",
    "    print(\"5. Run cell ini lagi\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b536766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üìö IMPORT LIBRARIES\n",
    "# ============================================================\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB4\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìö LIBRARIES LOADED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n‚úÖ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"‚úÖ GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12f6ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ‚öôÔ∏è CONFIGURATION\n",
    "# ============================================================\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS_PHASE1 = 50\n",
    "EPOCHS_PHASE2 = 50\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üöÄ TRAINING CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚ö° Strategy:\")\n",
    "print(\"   ‚Ä¢ Dataset: 21,779 images (37 classes, ~470 per class)\")\n",
    "print(\"   ‚Ä¢ Model: EfficientNetB4 (ImageNet pretrained)\")\n",
    "print(\"   ‚Ä¢ Training: 2-Phase (50+50 epochs)\")\n",
    "print(\"   ‚Ä¢ Phase 1: Freeze base, LR 1e-3\")\n",
    "print(\"   ‚Ä¢ Phase 2: Fine-tune all, LR 1e-4\")\n",
    "print(\"   ‚Ä¢ Expected: 95-99% accuracy\")\n",
    "print(\"   ‚Ä¢ Time: 6-8 hours total\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5343629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üîÑ DATA AUGMENTATION\n",
    "# ============================================================\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=180,\n",
    "    width_shift_range=0.5,\n",
    "    height_shift_range=0.5,\n",
    "    shear_range=0.5,\n",
    "    zoom_range=[0.4, 2.0],\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=[0.2, 2.0],\n",
    "    channel_shift_range=80,\n",
    "    fill_mode='reflect',\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Augmentation MAKSIMAL activated!\")\n",
    "print(\"   Akan generate 5000+ variasi per class during training!\")\n",
    "print(\"   = Total ~185,000+ training samples per epoch! üöÄ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ee6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üìÇ LOAD DATASET\n",
    "# ============================================================\n",
    "dataset_path = '/kaggle/working/batik_ultimate'\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìÇ LOADING DATASET\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìÇ Dataset path: {dataset_path}\\n\")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "num_classes = train_generator.num_classes\n",
    "images_per_class = train_generator.samples // num_classes\n",
    "\n",
    "print(f\"\\n‚úÖ Data loaded successfully!\")\n",
    "print(f\"   Jumlah kelas: {num_classes}\")\n",
    "print(f\"   Training samples: {train_generator.samples}\")\n",
    "print(f\"   Validation samples: {val_generator.samples}\")\n",
    "print(f\"   Images per class: ~{images_per_class}\")\n",
    "print(f\"   Class names: {list(train_generator.class_indices.keys())[:5]}... (showing first 5)\")\n",
    "\n",
    "if images_per_class > 400:\n",
    "    print(\"\\nüéâüéâüéâ PHENOMENAL! 400+ per class!\")\n",
    "    print(\"   Expected accuracy: 95-99%+ üöÄüöÄüöÄ\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93243d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üìä CALCULATE TRAINING STEPS\n",
    "# ============================================================\n",
    "original_per_class = train_generator.samples // num_classes\n",
    "\n",
    "# ADAPTIVE multiplier\n",
    "if original_per_class > 300:\n",
    "    multiplier = 2\n",
    "    print(\"‚úÖ Dataset is LARGE - using 2x multiplier\")\n",
    "else:\n",
    "    multiplier = 3\n",
    "    print(\"‚úÖ Using 3x multiplier\")\n",
    "\n",
    "steps_per_epoch = (train_generator.samples // BATCH_SIZE) * multiplier\n",
    "\n",
    "print(f\"\\nüìä TRAINING STATISTICS:\")\n",
    "print(f\"   - Jumlah kelas: {num_classes}\")\n",
    "print(f\"   - Images per class: ~{original_per_class}\")\n",
    "print(f\"   - Training samples: {train_generator.samples}\")\n",
    "print(f\"   - Multiplier: {multiplier}x\")\n",
    "print(f\"   - Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"   - Time per epoch: ~4 minutes\")\n",
    "print(f\"\\nüéØ Expected: 95-99% accuracy\")\n",
    "print(f\"‚è±Ô∏è  Total training time: ~6-8 hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4cf7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üèóÔ∏è BUILD MODEL\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üèóÔ∏è  BUILDING MODEL - EfficientNetB4\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "base_model = EfficientNetB4(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(*IMG_SIZE, 3)\n",
    ")\n",
    "\n",
    "# Freeze base model dulu\n",
    "base_model.trainable = False\n",
    "\n",
    "# Build model\n",
    "inputs = base_model.input\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "print(f\"‚úÖ Model created!\")\n",
    "print(f\"   Total parameters: {model.count_params():,}\")\n",
    "print(f\"   Trainable parameters: {sum([tf.size(w).numpy() for w in model.trainable_weights]):,}\")\n",
    "print(f\"   Non-trainable parameters: {sum([tf.size(w).numpy() for w in model.non_trainable_weights]):,}\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8430cf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ‚öôÔ∏è COMPILE MODEL - PHASE 1\n",
    "# ============================================================\n",
    "model.compile(\n",
    "    optimizer=AdamW(learning_rate=1e-3),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model compiled!\")\n",
    "print(\"   Optimizer: AdamW\")\n",
    "print(\"   Learning rate: 1e-3 (Phase 1)\")\n",
    "print(\"   Loss: categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add770e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üìã CALLBACKS\n",
    "# ============================================================\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        'best_model_batik.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=20,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=7,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Callbacks configured:\")\n",
    "print(\"   - ModelCheckpoint: Save best model\")\n",
    "print(\"   - EarlyStopping: Stop if no improvement (patience=20)\")\n",
    "print(\"   - ReduceLROnPlateau: Reduce LR if plateau (patience=7)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7d4707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üî• PHASE 1: TRAIN TOP LAYERS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìà PHASE 1: Training Top Layers (Base Frozen)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n‚è±Ô∏è  Estimated time: ~3-4 hours\")\n",
    "print(f\"üìä Epochs: {EPOCHS_PHASE1}\")\n",
    "print(f\"üéØ Target: Learn general patterns\\n\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "history1 = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=EPOCHS_PHASE1,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ PHASE 1 COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43310f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üîì UNFREEZE BASE MODEL FOR PHASE 2\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîì UNFREEZING BASE MODEL\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Unfreeze base model\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, tf.keras.Model) and 'efficientnet' in layer.name.lower():\n",
    "        layer.trainable = True\n",
    "        print(f\"‚úÖ Unfrozen: {layer.name}\")\n",
    "        break\n",
    "\n",
    "# Recompile with lower learning rate\n",
    "model.compile(\n",
    "    optimizer=AdamW(learning_rate=1e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Model recompiled!\")\n",
    "print(f\"   Learning rate: 1e-4 (10x lower for fine-tuning)\")\n",
    "print(f\"   Trainable parameters: {sum([tf.size(w).numpy() for w in model.trainable_weights]):,}\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c22f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üöÄ PHASE 2: FINE-TUNE ALL LAYERS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üî• PHASE 2: Fine-Tuning All Layers\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n‚è±Ô∏è  Estimated time: ~3-4 hours\")\n",
    "print(f\"üìä Epochs: {EPOCHS_PHASE2}\")\n",
    "print(f\"üéØ Target: Achieve 95-99% accuracy\\n\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "history2 = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=EPOCHS_PHASE1 + EPOCHS_PHASE2,\n",
    "    initial_epoch=EPOCHS_PHASE1,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ PHASE 2 COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d01c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üíæ SAVE FINAL MODEL\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üíæ SAVING MODELS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Save final model\n",
    "model.save('final_model_batik.keras')\n",
    "model.save('final_model_batik.h5')\n",
    "print(\"‚úÖ Saved: final_model_batik.keras\")\n",
    "print(\"‚úÖ Saved: final_model_batik.h5\")\n",
    "\n",
    "# Save class names\n",
    "import json\n",
    "with open('class_names.json', 'w') as f:\n",
    "    json.dump(list(train_generator.class_indices.keys()), f, indent=2)\n",
    "print(\"‚úÖ Saved: class_names.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ ALL MODELS SAVED!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83928f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üìä EVALUATE FINAL ACCURACY\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéØ FINAL EVALUATION\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "final_loss, final_acc = model.evaluate(val_generator, verbose=0)\n",
    "\n",
    "print(f\"üìä Final Results:\")\n",
    "print(f\"   Validation Loss: {final_loss:.4f}\")\n",
    "print(f\"   Validation Accuracy: {final_acc*100:.2f}%\")\n",
    "\n",
    "if final_acc >= 0.99:\n",
    "    print(\"\\nüéâüéâüéâ CONGRATULATIONS! Accuracy 99%+ ACHIEVED!\")\n",
    "    print(\"   PERFECT MODEL! Production ready! üöÄüöÄüöÄ\")\n",
    "elif final_acc >= 0.95:\n",
    "    print(\"\\nüéâüéâ EXCELLENT! Accuracy 95%+!\")\n",
    "    print(\"   Model sangat baik untuk production! üöÄüöÄ\")\n",
    "elif final_acc >= 0.90:\n",
    "    print(\"\\nüéâ GREAT! Accuracy 90%+!\")\n",
    "    print(\"   Model bagus! üöÄ\")\n",
    "else:\n",
    "    print(\"\\nüí™ Keep training or adjust parameters for better accuracy!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7697c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üìà PLOT TRAINING HISTORY\n",
    "# ============================================================\n",
    "print(\"\\nüìà Plotting training history...\\n\")\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Combine histories\n",
    "acc = history1.history['accuracy'] + history2.history['accuracy']\n",
    "val_acc = history1.history['val_accuracy'] + history2.history['val_accuracy']\n",
    "loss = history1.history['loss'] + history2.history['loss']\n",
    "val_loss = history1.history['val_loss'] + history2.history['val_loss']\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(acc, label='Train Accuracy', linewidth=2)\n",
    "plt.plot(val_acc, label='Validation Accuracy', linewidth=2)\n",
    "plt.axvline(x=EPOCHS_PHASE1, color='r', linestyle='--', label='Phase 2 Start')\n",
    "plt.title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loss, label='Train Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.axvline(x=EPOCHS_PHASE1, color='r', linestyle='--', label='Phase 2 Start')\n",
    "plt.title('Model Loss', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Saved: training_history.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7b157b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üì• LIST OUTPUT FILES\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üì• OUTPUT FILES READY FOR DOWNLOAD\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "output_files = [\n",
    "    'best_model_batik.keras',\n",
    "    'final_model_batik.keras',\n",
    "    'final_model_batik.h5',\n",
    "    'class_names.json',\n",
    "    'training_history.png'\n",
    "]\n",
    "\n",
    "print(\"üì¶ Files generated:\")\n",
    "for file in output_files:\n",
    "    if os.path.exists(file):\n",
    "        size = os.path.getsize(file) / (1024 * 1024)  # MB\n",
    "        print(f\"   ‚úÖ {file} ({size:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {file} (not found)\")\n",
    "\n",
    "print(\"\\nüí° To download:\")\n",
    "print(\"   1. Click on 'Output' tab on the right\")\n",
    "print(\"   2. Click download icon next to each file\")\n",
    "print(\"   3. Or download all at once after notebook completes\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ TRAINING COMPLETE! MODEL READY FOR PRODUCTION! üöÄ\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
